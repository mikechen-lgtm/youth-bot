"""
å°† RAG èµ„æ–™å¤¹ä¸­çš„ JSON æ¡£æ¡ˆæ±‡å…¥ MySQL èµ„æ–™è¡¨
èµ„æ–™è¡¨æ ä½ç›´æ¥å¯¹åº” JSON å­—æ®µï¼Œä¸è¿›è¡Œé¢å¤–è§£æ

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/json_to_database.py --rag-dir rag_data
"""
from __future__ import annotations

import argparse
import hashlib
import json
import logging
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from zoneinfo import ZoneInfo

# è½½å…¥ç¯å¢ƒå˜æ•°
load_dotenv()
load_dotenv(".env.local")

# è®¾å®šæ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

TAIPEI_TZ = ZoneInfo("Asia/Taipei")


def extract_facebook_id(url: str) -> Optional[str]:
    """
    å¾ Facebook URL æå–çœŸå¯¦çš„ Post ID

    æ”¯æ´çš„æ ¼å¼ï¼š
    1. pfbid æ ¼å¼ï¼šhttps://www.facebook.com/.../posts/pfbid0abc123...
    2. reel æ ¼å¼ï¼šhttps://www.facebook.com/reel/123456789/
    3. posts æ•¸å­—æ ¼å¼ï¼šhttps://www.facebook.com/.../posts/123456789
    """
    if not url:
        return None

    match = re.search(r'pfbid[0-9a-zA-Z]+', url)
    if match:
        return match.group(0)

    match = re.search(r'/reel/(\d+)', url)
    if match:
        return f"reel_{match.group(1)}"

    match = re.search(r'/posts/(\d+)', url)
    if match:
        return f"post_{match.group(1)}"

    return None


def resolve_post_id(post: dict, source: str) -> str:
    """
    æ±ºå®šè²¼æ–‡çš„å”¯ä¸€ post_id

    å„ªå…ˆé †åºï¼š
    1. JSON id å·²ç¶“æ˜¯ pfbid/reel_/post_ å­—ä¸² â†’ ç›´æ¥ä½¿ç”¨
    2. å¾ URL æå– Facebook ID
    3. Fallback: ç”¨ source + url ç”¢ç”Ÿç¢ºå®šæ€§ hash
    """
    json_id = post.get("id")

    if isinstance(json_id, str) and (
        json_id.startswith("pfbid")
        or json_id.startswith("reel_")
        or json_id.startswith("post_")
    ):
        return json_id

    url = post.get("url", "")
    fb_id = extract_facebook_id(url)
    if fb_id:
        return fb_id

    hash_input = f"{source}:{url or post.get('title', '')}"
    short_hash = hashlib.md5(hash_input.encode("utf-8")).hexdigest()[:16]
    logger.warning(
        f"ç„¡æ³•å¾ URL æå– Facebook ID: '{post.get('title', '?')[:40]}', "
        f"ä½¿ç”¨ fallback hash: fallback_{short_hash}"
    )
    return f"fallback_{short_hash}"


def _build_mysql_url() -> str:
    """å»ºç«‹ MySQL è¿æ¥ URL"""
    if os.getenv("MYSQL_URL"):
        return os.getenv("MYSQL_URL")

    host = os.getenv("MYSQL_HOST", "localhost")
    port = os.getenv("MYSQL_PORT", "3306")
    user = os.getenv("MYSQL_USER", "root")
    password = os.getenv("MYSQL_PASSWORD", "")
    database = os.getenv("MYSQL_DATABASE", "youth-chat")
    return f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"


def create_engine_instance() -> Engine:
    """åˆ›å»º SQLAlchemy engine"""
    mysql_url = _build_mysql_url()
    return create_engine(
        mysql_url,
        pool_pre_ping=True,
        pool_size=5,
        max_overflow=10,
        pool_recycle=3600,
        connect_args={"charset": "utf8mb4"},
    )


def ensure_activities_table(engine: Engine) -> None:
    """ç¡®ä¿ fb_activities èµ„æ–™è¡¨å­˜åœ¨ï¼ˆç®€åŒ–ç‰ˆï¼ŒåªåŒ…å« JSON å®é™…å­—æ®µï¼‰"""
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS fb_activities (
        id INT AUTO_INCREMENT PRIMARY KEY COMMENT 'èµ„æ–™è¡¨ä¸»é”®',
        source VARCHAR(100) NOT NULL COMMENT 'æ¥æºï¼ˆä»æ¡£åæå–ï¼‰',
        post_id VARCHAR(200) COMMENT 'Facebook è´´æ–‡ IDï¼ˆpfbid æˆ–æ•°å­—å­—ä¸²ï¼‰',
        title VARCHAR(500) NOT NULL COMMENT 'æ ‡é¢˜',
        content TEXT COMMENT 'å†…å®¹',
        publish_date DATETIME COMMENT 'å‘å¸ƒæ—¥æœŸ',
        event_date DATETIME COMMENT 'æ´»å‹•æ—¥æœŸï¼ˆå¾å…§å®¹æå–ï¼‰',
        url VARCHAR(1000) COMMENT 'åŸæ–‡è¿ç»“',
        tags JSON COMMENT 'æ ‡ç­¾ï¼ˆJSON é˜µåˆ—ï¼‰',
        retrieval_time DATETIME COMMENT 'çˆ¬å–æ—¶é—´',
        raw_data JSON COMMENT 'å®Œæ•´åŸå§‹èµ„æ–™ï¼ˆJSONï¼‰',
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
        updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
        INDEX idx_source (source),
        INDEX idx_title (title(100)),
        INDEX idx_publish_date (publish_date),
        UNIQUE KEY unique_post (source, post_id)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
    COMMENT='Facebook è´´æ–‡èµ„æ–™è¡¨ï¼ˆå¯¹åº” JSON å­—æ®µï¼‰'
    """

    with engine.begin() as conn:
        conn.execute(text(create_table_sql))

    # è¿ç§»ï¼šå¦‚æœ post_id ä»æ˜¯ INT ç±»å‹ï¼Œæ”¹ä¸º VARCHAR(200)
    migrate_sql = """
    SELECT DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS
    WHERE TABLE_SCHEMA = DATABASE()
      AND TABLE_NAME = 'fb_activities'
      AND COLUMN_NAME = 'post_id'
    """
    with engine.begin() as conn:
        result = conn.execute(text(migrate_sql))
        row = result.fetchone()
        if row and row[0].lower() == "int":
            logger.info("âš¡ æ£€æµ‹åˆ° post_id ä¸º INTï¼Œæ­£åœ¨è¿ç§»ä¸º VARCHAR(200)...")
            conn.execute(text("ALTER TABLE fb_activities DROP INDEX unique_post"))
            conn.execute(text(
                "ALTER TABLE fb_activities MODIFY COLUMN post_id VARCHAR(200) "
                "COMMENT 'Facebook è´´æ–‡ IDï¼ˆpfbid æˆ–æ•°å­—å­—ä¸²ï¼‰'"
            ))
            conn.execute(text(
                "ALTER TABLE fb_activities ADD UNIQUE KEY unique_post (source, post_id)"
            ))
            logger.info("âœ“ post_id å·²è¿ç§»ä¸º VARCHAR(200)")

    # è¿ç§»ï¼šæ–°å¢ event_date æ¬„ä½ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    check_event_date_sql = """
    SELECT COUNT(*) FROM INFORMATION_SCHEMA.COLUMNS
    WHERE TABLE_SCHEMA = DATABASE()
      AND TABLE_NAME = 'fb_activities'
      AND COLUMN_NAME = 'event_date'
    """
    with engine.begin() as conn:
        result = conn.execute(text(check_event_date_sql))
        row = result.fetchone()
        if row and row[0] == 0:
            logger.info("âš¡ æ–°å¢ event_date æ¬„ä½...")
            conn.execute(text(
                "ALTER TABLE fb_activities ADD COLUMN event_date DATETIME "
                "COMMENT 'æ´»å‹•æ—¥æœŸï¼ˆå¾å…§å®¹æå–ï¼‰' AFTER publish_date"
            ))
            conn.execute(text(
                "CREATE INDEX idx_event_date ON fb_activities (event_date)"
            ))
            logger.info("âœ“ event_date æ¬„ä½èˆ‡ç´¢å¼•å·²æ–°å¢")

    logger.info("âœ“ èµ„æ–™è¡¨ 'fb_activities' å·²ç¡®ä¿å­˜åœ¨")


def clear_activities_table(engine: Engine) -> None:
    """æ¸…ç©º fb_activities èµ„æ–™è¡¨ï¼ˆç”¨äºå®Œå…¨é‡å»ºï¼‰"""
    truncate_sql = "TRUNCATE TABLE fb_activities"

    with engine.begin() as conn:
        conn.execute(text(truncate_sql))

    logger.info("âœ“ èµ„æ–™è¡¨ 'fb_activities' å·²æ¸…ç©º")


def parse_datetime(date_str: str) -> Optional[datetime]:
    """è§£ææ—¥æœŸæ—¶é—´å­—ä¸²"""
    if not date_str:
        return None

    try:
        if "T" in date_str:
            dt = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
            return dt.astimezone(TAIPEI_TZ)
    except (ValueError, TypeError):
        pass

    return None


def extract_event_date(content: str, publish_date: Optional[datetime] = None) -> Optional[datetime]:
    """
    å¾è²¼æ–‡å…§å®¹ä¸­æå–æ´»å‹•æ—¥æœŸ

    ä½¿ç”¨ regex è§£æå¸¸è¦‹çš„ä¸­æ–‡æ—¥æœŸæ ¼å¼ï¼Œå›å‚³æœ€æ—©ä¸” >= publish_date çš„æ—¥æœŸã€‚
    è‹¥ç„¡æœªä¾†æ—¥æœŸï¼Œå›å‚³æ‰€æœ‰å€™é¸ä¸­æœ€æ—©çš„æ—¥æœŸã€‚ç„¡æ³•æå–æ™‚å›å‚³ Noneã€‚

    æ”¯æ´æ ¼å¼ï¼š
    - æ°‘åœ‹å¹´å…¨ç¨±ï¼š115å¹´02æœˆ11æ—¥
    - æ°‘åœ‹å¹´æ–œç·šï¼š115/03/01
    - è¥¿å…ƒå¹´ï¼š2026/02/11
    - ä¸­æ–‡æœˆæ—¥ï¼š2æœˆ11æ—¥ï¼ˆå¾ publish_date æ¨æ–·å¹´ä»½ï¼‰
    - MM/DD(æ˜ŸæœŸ)ï¼š02/11(ä¸‰)ï¼ˆå¾ publish_date æ¨æ–·å¹´ä»½ï¼‰
    """
    if not content:
        return None

    candidates = []
    ref_year = publish_date.year if publish_date else datetime.now(TAIPEI_TZ).year

    # Pattern 1: æ°‘åœ‹å¹´å…¨ç¨± â€” 115å¹´02æœˆ11æ—¥ or 115å¹´2æœˆ11æ—¥
    for m in re.finditer(r'(\d{2,3})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥', content):
        try:
            year = int(m.group(1)) + 1911
            month = int(m.group(2))
            day = int(m.group(3))
            if 2020 <= year <= 2030 and 1 <= month <= 12 and 1 <= day <= 31:
                candidates.append(datetime(year, month, day, tzinfo=TAIPEI_TZ))
        except (ValueError, OverflowError):
            continue

    # Pattern 2: æ°‘åœ‹å¹´æ–œç·š â€” 115/03/01ï¼ˆé¦–æ®µ 100~200 æ‰è¦–ç‚ºæ°‘åœ‹å¹´ï¼‰
    for m in re.finditer(r'(?<!\d)(\d{3})/(\d{1,2})/(\d{1,2})(?!\d)', content):
        try:
            roc_year = int(m.group(1))
            if 100 <= roc_year <= 200:
                year = roc_year + 1911
                month = int(m.group(2))
                day = int(m.group(3))
                if 1 <= month <= 12 and 1 <= day <= 31:
                    candidates.append(datetime(year, month, day, tzinfo=TAIPEI_TZ))
        except (ValueError, OverflowError):
            continue

    # Pattern 3: è¥¿å…ƒå¹´ â€” 2026/02/11 or 2026/2/11
    for m in re.finditer(r'(?<!\d)(20\d{2})/(\d{1,2})/(\d{1,2})(?!\d)', content):
        try:
            year = int(m.group(1))
            month = int(m.group(2))
            day = int(m.group(3))
            if 2020 <= year <= 2030 and 1 <= month <= 12 and 1 <= day <= 31:
                candidates.append(datetime(year, month, day, tzinfo=TAIPEI_TZ))
        except (ValueError, OverflowError):
            continue

    # Pattern 4: ä¸­æ–‡æœˆæ—¥ï¼ˆç„¡å¹´ä»½ï¼‰â€” 2æœˆ11æ—¥ or 11æœˆ15æ—¥
    # æ’é™¤å·²è¢« Pattern 1 åŒ¹é…çš„ï¼ˆå‰é¢æœ‰ã€Œå¹´ã€å­—ï¼‰
    for m in re.finditer(r'(?<!\d)(?<!å¹´)(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥', content):
        try:
            month = int(m.group(1))
            day = int(m.group(2))
            if 1 <= month <= 12 and 1 <= day <= 31:
                dt = datetime(ref_year, month, day, tzinfo=TAIPEI_TZ)
                # è‹¥æ¯” publish_date æ—©è¶…é 60 å¤©ï¼Œå¯èƒ½æ˜¯è·¨å¹´ï¼Œè©¦ +1 å¹´
                if publish_date and (publish_date - dt).days > 60:
                    dt = datetime(ref_year + 1, month, day, tzinfo=TAIPEI_TZ)
                candidates.append(dt)
        except (ValueError, OverflowError):
            continue

    # Pattern 5: MM/DD(æ˜ŸæœŸ) â€” 02/11(ä¸‰) or 6/3(äºŒ)
    for m in re.finditer(r'(?<!\d)(\d{1,2})/(\d{1,2})\s*[\(ï¼ˆ][ä¸€äºŒä¸‰å››äº”å…­æ—¥][\)ï¼‰]', content):
        try:
            month = int(m.group(1))
            day = int(m.group(2))
            if 1 <= month <= 12 and 1 <= day <= 31:
                dt = datetime(ref_year, month, day, tzinfo=TAIPEI_TZ)
                if publish_date and (publish_date - dt).days > 60:
                    dt = datetime(ref_year + 1, month, day, tzinfo=TAIPEI_TZ)
                candidates.append(dt)
        except (ValueError, OverflowError):
            continue

    if not candidates:
        return None

    # å»é‡
    candidates = list(set(candidates))

    # å„ªå…ˆé¸å– >= publish_date çš„æœ€æ—©æ—¥æœŸ
    if publish_date:
        pub_date_naive = publish_date.replace(hour=0, minute=0, second=0, microsecond=0)
        future_dates = [d for d in candidates if d >= pub_date_naive]
        if future_dates:
            return min(future_dates)

    # æ²’æœ‰æœªä¾†æ—¥æœŸï¼Œå›å‚³æœ€æ—©çš„æ—¥æœŸ
    return min(candidates)


def prepare_activity_data(post: dict, source: str) -> Dict[str, Any]:
    """å‡†å¤‡è¦æ’å…¥èµ„æ–™è¡¨çš„æ´»åŠ¨èµ„æ–™ï¼ˆç›´æ¥å¯¹åº” JSON å­—æ®µï¼‰"""
    # è§£ææ—¥æœŸæ—¶é—´
    publish_date = parse_datetime(post.get("publish_date"))
    retrieval_time = parse_datetime(post.get("retrieval_time"))

    # å¾å…§å®¹æå–æ´»å‹•æ—¥æœŸ
    content = (post.get("content") or "").strip()
    event_date = extract_event_date(content, publish_date)

    # å‡†å¤‡ JSON æ ä½
    tags = json.dumps(post.get("tags"), ensure_ascii=False) if post.get("tags") else None
    raw_data = json.dumps(post, ensure_ascii=False)

    # ä½¿ç”¨ resolve_post_id å–å¾—æ­£ç¡®çš„ Facebook è´´æ–‡ ID
    post_id = resolve_post_id(post, source)

    return {
        "source": source,
        "post_id": post_id,
        "title": post.get("title", "æ— æ ‡é¢˜")[:500],
        "content": content,
        "publish_date": publish_date,
        "event_date": event_date,
        "url": post.get("url"),
        "tags": tags,
        "retrieval_time": retrieval_time,
        "raw_data": raw_data,
    }


def insert_activity(engine: Engine, activity_data: Dict[str, Any]) -> bool:
    """æ’å…¥æˆ–æ›´æ–°æ´»åŠ¨èµ„æ–™"""
    insert_sql = """
    INSERT INTO fb_activities (
        source, post_id, title, content, publish_date, event_date, url, tags, retrieval_time, raw_data
    ) VALUES (
        :source, :post_id, :title, :content, :publish_date, :event_date, :url, :tags, :retrieval_time, :raw_data
    )
    ON DUPLICATE KEY UPDATE
        title = VALUES(title),
        content = VALUES(content),
        publish_date = VALUES(publish_date),
        event_date = VALUES(event_date),
        url = VALUES(url),
        tags = VALUES(tags),
        retrieval_time = VALUES(retrieval_time),
        raw_data = VALUES(raw_data),
        updated_at = CURRENT_TIMESTAMP
    """

    try:
        with engine.begin() as conn:
            conn.execute(text(insert_sql), activity_data)
        return True
    except Exception as e:
        logger.error(f"æ’å…¥æ´»åŠ¨å¤±è´¥: {activity_data.get('title')[:50]} - {e}")
        return False


def process_json_files(rag_dir: Path, engine: Engine) -> Dict[str, int]:
    """å¤„ç†æ‰€æœ‰ JSON æ¡£æ¡ˆ"""
    json_files = list(rag_dir.glob("FB-POST-*.json"))

    # æŒ‰ç…§æ¡£åæ­£åºæ’åºï¼Œç¡®ä¿è¾ƒæ–°çš„æ¡£æ¡ˆåå¤„ç†ï¼ˆé¿å…æ—§èµ„æ–™è¦†ç›–æ–°èµ„æ–™ï¼‰
    # æ¡£åæ ¼å¼ï¼šFB-POST-æ¥æº-YYYYMMDD.jsonï¼ˆç»Ÿä¸€ä½¿ç”¨è¥¿å…ƒå¹´ï¼‰
    # - 20260121 < 20260129ï¼ˆå­—æ¯é¡ºåº = æ—¶é—´é¡ºåºï¼‰
    # æ­£åºåï¼š20260121 æ¡£æ¡ˆå…ˆå¤„ç†ï¼Œ20260129 æ¡£æ¡ˆåå¤„ç†ï¼ˆæ–°èµ„æ–™è¦†ç›–æ—§èµ„æ–™ï¼‰
    json_files.sort(key=lambda x: x.name)

    stats = {
        "total_files": len(json_files),
        "total_posts": 0,
        "imported": 0,
        "failed": 0,
    }

    logger.info(f"ğŸ“‚ æ‰¾åˆ° {len(json_files)} ä¸ª JSON æ¡£æ¡ˆ")
    logger.info(f"ğŸ“‹ å¤„ç†é¡ºåºï¼ˆæŒ‰æ¡£åæ­£åºæ’åˆ—ï¼Œç¡®ä¿æ–°æ¡£æ¡ˆåå¤„ç†ï¼‰ï¼š")
    for i, f in enumerate(json_files, 1):
        logger.info(f"  {i}. {f.name}")

    for json_path in json_files:
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            if "posts" not in data:
                logger.info(f"  â­ï¸  è·³è¿‡ {json_path.name}ï¼ˆä¸åŒ…å« postsï¼‰")
                continue

            source = data.get("source", json_path.stem)
            posts = data.get("posts", [])
            stats["total_posts"] += len(posts)

            logger.info(f"\nå¤„ç†æ¥æºï¼š{source}ï¼ˆ{len(posts)} ä¸ªè´´æ–‡ï¼‰")

            for post in posts:
                title = (post.get("title") or "").strip()
                content = (post.get("content") or "").strip()

                # è·³éæ¨™é¡Œå’Œå…§å®¹éƒ½ç‚ºç©ºçš„è²¼æ–‡
                if not title and not content:
                    logger.info(f"  â­ï¸  è·³éç©ºè²¼æ–‡ (id={post.get('id', 'N/A')})")
                    continue

                title_display = title[:50] or content[:50]

                # å‡†å¤‡æ´»åŠ¨èµ„æ–™
                activity_data = prepare_activity_data(post, source)

                # æ’å…¥èµ„æ–™åº“
                if insert_activity(engine, activity_data):
                    stats["imported"] += 1
                    post_id = post.get("id", "N/A")
                    logger.info(f"  âœ“ [{post_id}] {title_display}")
                else:
                    stats["failed"] += 1

        except json.JSONDecodeError as e:
            logger.error(f"  âŒ JSON è§£æé”™è¯¯ {json_path.name}: {e}")
            stats["failed"] += 1
        except Exception as e:
            logger.error(f"  âŒ å¤„ç†é”™è¯¯ {json_path.name}: {e}")
            stats["failed"] += 1

    return stats


def print_statistics(stats: Dict[str, int]) -> None:
    """åˆ—å°ç»Ÿè®¡èµ„è®¯"""
    print("\n" + "="*60)
    print("ğŸ“Š æ±‡å…¥ç»Ÿè®¡")
    print("="*60)
    print(f"å¤„ç†æ¡£æ¡ˆæ•°ï¼š{stats['total_files']}")
    print(f"è´´æ–‡æ€»æ•°ï¼š{stats['total_posts']}")
    print(f"âœ… æˆåŠŸæ±‡å…¥ï¼š{stats['imported']}")
    print(f"âŒ å¤±è´¥ï¼š{stats['failed']}")
    print("="*60)


def query_activities_summary(engine: Engine) -> None:
    """æŸ¥è¯¢å¹¶æ˜¾ç¤ºæ´»åŠ¨èµ„æ–™æ‘˜è¦"""
    summary_sql = """
    SELECT
        source,
        COUNT(*) as total,
        MIN(publish_date) as earliest,
        MAX(publish_date) as latest,
        SUM(CASE WHEN event_date IS NOT NULL THEN 1 ELSE 0 END) as with_event_date
    FROM fb_activities
    GROUP BY source
    ORDER BY source
    """

    print("\n" + "="*100)
    print("ğŸ“‹ èµ„æ–™è¡¨æ‘˜è¦")
    print("="*100)

    with engine.begin() as conn:
        result = conn.execute(text(summary_sql))
        rows = result.fetchall()

        if not rows:
            print("èµ„æ–™è¡¨ä¸ºç©º")
            return

        print(f"{'æ¥æº':<40} {'æ€»æ•°':>6} {'æœ‰æ´»å‹•æ—¥æœŸ':>10} {'æœ€æ—©å‘å¸ƒ':>14} {'æœ€æ™šå‘å¸ƒ':>14}")
        print("-"*100)

        total_all = 0
        total_event_date = 0

        for row in rows:
            source, total, earliest, latest, with_event_date = row
            total_all += total
            total_event_date += with_event_date

            earliest_str = earliest.strftime("%Y-%m-%d") if earliest else "N/A"
            latest_str = latest.strftime("%Y-%m-%d") if latest else "N/A"

            print(f"{source:<40} {total:>6} {with_event_date:>10} {earliest_str:>14} {latest_str:>14}")

        print("-"*100)
        print(f"{'æ€»è®¡':<40} {total_all:>6} {total_event_date:>10}")

    print("="*100)


def main() -> int:
    parser = argparse.ArgumentParser(
        description="å°† JSON è´´æ–‡èµ„æ–™æ±‡å…¥ MySQL èµ„æ–™è¡¨ï¼ˆå¯¹åº” JSON å­—æ®µï¼‰"
    )
    parser.add_argument(
        "--rag-dir",
        default="rag_data",
        help="RAG èµ„æ–™ç›®å½•ï¼ˆé¢„è®¾: rag_dataï¼‰",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…å†™å…¥èµ„æ–™åº“ï¼‰",
    )
    parser.add_argument(
        "--clear-table",
        action="store_true",
        help="æ¸…ç©ºèµ„æ–™è¡¨åå†æ±‡å…¥ï¼ˆç”¨äºå®Œå…¨é‡å»ºï¼‰",
    )

    args = parser.parse_args()

    rag_dir = Path(args.rag_dir)
    if not rag_dir.exists():
        logger.error(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° RAG ç›®å½•ï¼š{rag_dir}")
        return 1

    if args.dry_run:
        logger.info("ğŸ” è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸ä¼šå†™å…¥èµ„æ–™åº“ï¼‰")
        return 0

    try:
        # åˆ›å»ºèµ„æ–™åº“è¿çº¿
        logger.info("ğŸ”— è¿æ¥èµ„æ–™åº“...")
        engine = create_engine_instance()

        # ç¡®ä¿èµ„æ–™è¡¨å­˜åœ¨
        ensure_activities_table(engine)

        # å¦‚æœéœ€è¦æ¸…ç©ºè¡¨
        if args.clear_table:
            logger.info("ğŸ—‘ï¸  æ¸…ç©ºç°æœ‰èµ„æ–™...")
            clear_activities_table(engine)

        # å¤„ç† JSON æ¡£æ¡ˆ
        logger.info(f"\nğŸš€ å¼€å§‹å¤„ç†...")
        logger.info(f"   è¾“å…¥ç›®å½•ï¼š{rag_dir}\n")

        stats = process_json_files(rag_dir, engine)

        # åˆ—å°ç»Ÿè®¡
        print_statistics(stats)

        # æ˜¾ç¤ºèµ„æ–™è¡¨æ‘˜è¦
        query_activities_summary(engine)

        logger.info("\nâœ¨ å®Œæˆï¼")
        return 0

    except Exception as e:
        logger.error(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
