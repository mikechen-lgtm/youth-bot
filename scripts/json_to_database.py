"""
å°† RAG èµ„æ–™å¤¹ä¸­çš„ JSON æ¡£æ¡ˆæ±‡å…¥ MySQL èµ„æ–™è¡¨
èµ„æ–™è¡¨æ ä½ç›´æ¥å¯¹åº” JSON å­—æ®µï¼Œä¸è¿›è¡Œé¢å¤–è§£æ

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/json_to_database.py --rag-dir rag_data
"""
from __future__ import annotations

import argparse
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from zoneinfo import ZoneInfo

# è½½å…¥ç¯å¢ƒå˜æ•°
load_dotenv()
load_dotenv(".env.local")

# è®¾å®šæ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

TAIPEI_TZ = ZoneInfo("Asia/Taipei")


def _build_mysql_url() -> str:
    """å»ºç«‹ MySQL è¿æ¥ URL"""
    if os.getenv("MYSQL_URL"):
        return os.getenv("MYSQL_URL")

    host = os.getenv("MYSQL_HOST", "localhost")
    port = os.getenv("MYSQL_PORT", "3306")
    user = os.getenv("MYSQL_USER", "root")
    password = os.getenv("MYSQL_PASSWORD", "")
    database = os.getenv("MYSQL_DATABASE", "youth-chat")
    return f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"


def create_engine_instance() -> Engine:
    """åˆ›å»º SQLAlchemy engine"""
    mysql_url = _build_mysql_url()
    return create_engine(
        mysql_url,
        pool_pre_ping=True,
        pool_size=5,
        max_overflow=10,
        pool_recycle=3600,
        connect_args={"charset": "utf8mb4"},
    )


def ensure_activities_table(engine: Engine) -> None:
    """ç¡®ä¿ fb_activities èµ„æ–™è¡¨å­˜åœ¨ï¼ˆç®€åŒ–ç‰ˆï¼ŒåªåŒ…å« JSON å®é™…å­—æ®µï¼‰"""
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS fb_activities (
        id INT AUTO_INCREMENT PRIMARY KEY COMMENT 'èµ„æ–™è¡¨ä¸»é”®',
        source VARCHAR(100) NOT NULL COMMENT 'æ¥æºï¼ˆä»æ¡£åæå–ï¼‰',
        post_id INT COMMENT 'JSON ä¸­çš„ post ID',
        title VARCHAR(500) NOT NULL COMMENT 'æ ‡é¢˜',
        content TEXT COMMENT 'å†…å®¹',
        publish_date DATETIME COMMENT 'å‘å¸ƒæ—¥æœŸ',
        url VARCHAR(1000) COMMENT 'åŸæ–‡è¿ç»“',
        tags JSON COMMENT 'æ ‡ç­¾ï¼ˆJSON é˜µåˆ—ï¼‰',
        retrieval_time DATETIME COMMENT 'çˆ¬å–æ—¶é—´',
        raw_data JSON COMMENT 'å®Œæ•´åŸå§‹èµ„æ–™ï¼ˆJSONï¼‰',
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
        updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
        INDEX idx_source (source),
        INDEX idx_title (title(100)),
        INDEX idx_publish_date (publish_date),
        UNIQUE KEY unique_post (source, post_id)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
    COMMENT='Facebook è´´æ–‡èµ„æ–™è¡¨ï¼ˆå¯¹åº” JSON å­—æ®µï¼‰'
    """

    with engine.begin() as conn:
        conn.execute(text(create_table_sql))

    logger.info("âœ“ èµ„æ–™è¡¨ 'fb_activities' å·²ç¡®ä¿å­˜åœ¨")


def parse_datetime(date_str: str) -> Optional[datetime]:
    """è§£ææ—¥æœŸæ—¶é—´å­—ä¸²"""
    if not date_str:
        return None

    try:
        if "T" in date_str:
            dt = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
            return dt.astimezone(TAIPEI_TZ)
    except (ValueError, TypeError):
        pass

    return None


def prepare_activity_data(post: dict, source: str) -> Dict[str, Any]:
    """å‡†å¤‡è¦æ’å…¥èµ„æ–™è¡¨çš„æ´»åŠ¨èµ„æ–™ï¼ˆç›´æ¥å¯¹åº” JSON å­—æ®µï¼‰"""
    # è§£ææ—¥æœŸæ—¶é—´
    publish_date = parse_datetime(post.get("publish_date"))
    retrieval_time = parse_datetime(post.get("retrieval_time"))

    # å‡†å¤‡ JSON æ ä½
    tags = json.dumps(post.get("tags"), ensure_ascii=False) if post.get("tags") else None
    raw_data = json.dumps(post, ensure_ascii=False)

    return {
        "source": source,
        "post_id": post.get("id"),
        "title": post.get("title", "æ— æ ‡é¢˜")[:500],
        "content": (post.get("content") or "").strip(),
        "publish_date": publish_date,
        "url": post.get("url"),
        "tags": tags,
        "retrieval_time": retrieval_time,
        "raw_data": raw_data,
    }


def insert_activity(engine: Engine, activity_data: Dict[str, Any]) -> bool:
    """æ’å…¥æˆ–æ›´æ–°æ´»åŠ¨èµ„æ–™"""
    insert_sql = """
    INSERT INTO fb_activities (
        source, post_id, title, content, publish_date, url, tags, retrieval_time, raw_data
    ) VALUES (
        :source, :post_id, :title, :content, :publish_date, :url, :tags, :retrieval_time, :raw_data
    )
    ON DUPLICATE KEY UPDATE
        title = VALUES(title),
        content = VALUES(content),
        publish_date = VALUES(publish_date),
        url = VALUES(url),
        tags = VALUES(tags),
        retrieval_time = VALUES(retrieval_time),
        raw_data = VALUES(raw_data),
        updated_at = CURRENT_TIMESTAMP
    """

    try:
        with engine.begin() as conn:
            conn.execute(text(insert_sql), activity_data)
        return True
    except Exception as e:
        logger.error(f"æ’å…¥æ´»åŠ¨å¤±è´¥: {activity_data.get('title')[:50]} - {e}")
        return False


def process_json_files(rag_dir: Path, engine: Engine) -> Dict[str, int]:
    """å¤„ç†æ‰€æœ‰ JSON æ¡£æ¡ˆ"""
    json_files = list(rag_dir.glob("FB-POST-*.json"))

    stats = {
        "total_files": len(json_files),
        "total_posts": 0,
        "imported": 0,
        "failed": 0,
    }

    logger.info(f"ğŸ“‚ æ‰¾åˆ° {len(json_files)} ä¸ª JSON æ¡£æ¡ˆ")

    for json_path in json_files:
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            if "posts" not in data:
                logger.info(f"  â­ï¸  è·³è¿‡ {json_path.name}ï¼ˆä¸åŒ…å« postsï¼‰")
                continue

            source = data.get("source", json_path.stem)
            posts = data.get("posts", [])
            stats["total_posts"] += len(posts)

            logger.info(f"\nå¤„ç†æ¥æºï¼š{source}ï¼ˆ{len(posts)} ä¸ªè´´æ–‡ï¼‰")

            for post in posts:
                title = post.get("title", "æ— æ ‡é¢˜")[:50]

                # å‡†å¤‡æ´»åŠ¨èµ„æ–™
                activity_data = prepare_activity_data(post, source)

                # æ’å…¥èµ„æ–™åº“
                if insert_activity(engine, activity_data):
                    stats["imported"] += 1
                    post_id = post.get("id", "N/A")
                    logger.info(f"  âœ“ [{post_id}] {title}")
                else:
                    stats["failed"] += 1

        except json.JSONDecodeError as e:
            logger.error(f"  âŒ JSON è§£æé”™è¯¯ {json_path.name}: {e}")
            stats["failed"] += 1
        except Exception as e:
            logger.error(f"  âŒ å¤„ç†é”™è¯¯ {json_path.name}: {e}")
            stats["failed"] += 1

    return stats


def print_statistics(stats: Dict[str, int]) -> None:
    """åˆ—å°ç»Ÿè®¡èµ„è®¯"""
    print("\n" + "="*60)
    print("ğŸ“Š æ±‡å…¥ç»Ÿè®¡")
    print("="*60)
    print(f"å¤„ç†æ¡£æ¡ˆæ•°ï¼š{stats['total_files']}")
    print(f"è´´æ–‡æ€»æ•°ï¼š{stats['total_posts']}")
    print(f"âœ… æˆåŠŸæ±‡å…¥ï¼š{stats['imported']}")
    print(f"âŒ å¤±è´¥ï¼š{stats['failed']}")
    print("="*60)


def query_activities_summary(engine: Engine) -> None:
    """æŸ¥è¯¢å¹¶æ˜¾ç¤ºæ´»åŠ¨èµ„æ–™æ‘˜è¦"""
    summary_sql = """
    SELECT
        source,
        COUNT(*) as total,
        MIN(publish_date) as earliest,
        MAX(publish_date) as latest
    FROM fb_activities
    GROUP BY source
    ORDER BY source
    """

    print("\n" + "="*80)
    print("ğŸ“‹ èµ„æ–™è¡¨æ‘˜è¦")
    print("="*80)

    with engine.begin() as conn:
        result = conn.execute(text(summary_sql))
        rows = result.fetchall()

        if not rows:
            print("èµ„æ–™è¡¨ä¸ºç©º")
            return

        print(f"{'æ¥æº':<40} {'æ€»æ•°':>10} {'æœ€æ—©å‘å¸ƒ':>20} {'æœ€æ™šå‘å¸ƒ':>20}")
        print("-"*80)

        total_all = 0

        for row in rows:
            source, total, earliest, latest = row
            total_all += total

            earliest_str = earliest.strftime("%Y-%m-%d") if earliest else "N/A"
            latest_str = latest.strftime("%Y-%m-%d") if latest else "N/A"

            print(f"{source:<40} {total:>10} {earliest_str:>20} {latest_str:>20}")

        print("-"*80)
        print(f"{'æ€»è®¡':<40} {total_all:>10}")

    print("="*80)


def main() -> int:
    parser = argparse.ArgumentParser(
        description="å°† JSON è´´æ–‡èµ„æ–™æ±‡å…¥ MySQL èµ„æ–™è¡¨ï¼ˆå¯¹åº” JSON å­—æ®µï¼‰"
    )
    parser.add_argument(
        "--rag-dir",
        default="rag_data",
        help="RAG èµ„æ–™ç›®å½•ï¼ˆé¢„è®¾: rag_dataï¼‰",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…å†™å…¥èµ„æ–™åº“ï¼‰",
    )

    args = parser.parse_args()

    rag_dir = Path(args.rag_dir)
    if not rag_dir.exists():
        logger.error(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° RAG ç›®å½•ï¼š{rag_dir}")
        return 1

    if args.dry_run:
        logger.info("ğŸ” è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸ä¼šå†™å…¥èµ„æ–™åº“ï¼‰")
        return 0

    try:
        # åˆ›å»ºèµ„æ–™åº“è¿çº¿
        logger.info("ğŸ”— è¿æ¥èµ„æ–™åº“...")
        engine = create_engine_instance()

        # ç¡®ä¿èµ„æ–™è¡¨å­˜åœ¨
        ensure_activities_table(engine)

        # å¤„ç† JSON æ¡£æ¡ˆ
        logger.info(f"\nğŸš€ å¼€å§‹å¤„ç†...")
        logger.info(f"   è¾“å…¥ç›®å½•ï¼š{rag_dir}\n")

        stats = process_json_files(rag_dir, engine)

        # åˆ—å°ç»Ÿè®¡
        print_statistics(stats)

        # æ˜¾ç¤ºèµ„æ–™è¡¨æ‘˜è¦
        query_activities_summary(engine)

        logger.info("\nâœ¨ å®Œæˆï¼")
        return 0

    except Exception as e:
        logger.error(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
